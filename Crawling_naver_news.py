import requests
import urllib.parse
import json
import os
import re
from datetime import datetime
from tqdm import tqdm
# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜ ë° ì €ì¥
import pandas as pd
from datetime import datetime
import time

# API í‚¤ ì„¤ì •
client_id = "TSv_s3GbD90z6QRiBZUN" #ê°œì¸ ê±° ë°œê¸‰ë°›ì•„ì„œ ì‚¬ìš©
client_secret = "jA9u6XAoDh"

# ê²€ìƒ‰ì–´ ì„¤ì • ë° ì¸ì½”ë”©
# query = "ë²¡ìŠ¤ì¸í…”ë¦¬ì „ìŠ¤"  # ê²€ìƒ‰ì–´ë¥¼ "ë²¡ìŠ¤ì¸í…”ë¦¬ì „ìŠ¤"ë¡œ ë³€ê²½
# encoded_query = urllib.parse.quote(query)
corp_name_list = pd.read_csv("enterprise_df_14_utf8_data.csv")["ê¸°ì—…ëª…"].dropna().unique().tolist()

# í—¤ë” ì„¤ì •
headers = {
    "X-Naver-Client-Id": client_id,
    "X-Naver-Client-Secret": client_secret,
    "User-Agent": "Mozilla/5.0",  # Sometimes adding a user agent helps
}

all_results = []

# ë””ë²„ê¹…ìš© í—¤ë” ì¶œë ¥
print(f"Using headers: {headers}")

for corp_name in tqdm(corp_name_list, desc="ê¸°ì—…ë³„ ë‰´ìŠ¤ ê²€ìƒ‰", unit="ê¸°ì—…"):
    try:
        time.sleep(1) #ì„œë²„ ë¶€í•˜ ë°©ì§€
        query = urllib.parse.quote(corp_name)
        url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=10&start=1&sort=sim"
        #Api ìš”ì²­ë³´ë‚´ê¸°
        response = requests.get(url, headers=headers)
        
        # ì‘ë‹µ í™•ì¸ (200 ì •ìƒ, 201 ìƒì„± ì‹œ ì •ìƒ, 200/201 ë¹¼ê³  ë‹¤ ë¹„ì •ìƒ)
        if response.status_code == 200:
            # JSON ê²°ê³¼ íŒŒì‹±
            data = json.loads(response.text)
            # print(f"ì´ ê²€ìƒ‰ ê²°ê³¼: {data['total']}ê°œ")
            
            # ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
            # for idx, item in enumerate(data["items"], 1):
            #     print(f"\n[{idx}] {item['title']}")
            #     print(f"Original link: {item['originallink']}")
            #     print(f"ë§í¬: {item['link']}")
            #     print(f"ì„¤ëª…: {item['description']}")
            #     print(f"ë°œí–‰ì¼: {item['pubDate']}")
            
            # ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
            df = pd.DataFrame(data["items"])
            
            # ê¸°ì—…ëª… ì»¬ëŸ¼ ì¶”ê°€
            df["ê¸°ì—…ëª…"] = corp_name
            
            # HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ë¦¬
            if "title" in df.columns:
                # ëª¨ë“  HTML íƒœê·¸ë¥¼ ì •ê·œì‹ìœ¼ë¡œ ì œê±°
                df["title"] = df["title"].apply(lambda x: re.sub(r"<.*?>", "", x))
                df["description"] = df["description"].apply(
                    lambda x: re.sub(r"<.*?>", "", x)
                )
            
            # all_resultsì— ì¶”ê°€
            all_results.append(df)
            
        else:
            print(f"Error {response.status_code}: {response.reason}")
            print(f"Response body: {response.text}")
    except Exception as e:
        print(f"Exception occurred for '{corp_name}': {e}")

# ëª¨ë“  ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í•©ì¹˜ê¸°
if all_results:
    final_df = pd.concat(all_results, ignore_index=True)
    
    # ê¸°ì—…ëª… ì»¬ëŸ¼ì„ ë§¨ ì•ìœ¼ë¡œ ì´ë™
    cols = ['ê¸°ì—…ëª…'] + [col for col in final_df.columns if col != 'ê¸°ì—…ëª…']
    final_df = final_df[cols]
    
    # í˜„ì¬ ì‹œê°„ì„ íŒŒì¼ëª…ì— ì¶”ê°€í•˜ì—¬ ì €ì¥
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    # íŒŒì¼ ì´ë¦„ ì„¤ì • (ëª¨ë“  ê¸°ì—… ë°ì´í„°ê°€ ë“¤ì–´ìˆë‹¤ëŠ” ì˜ë¯¸ë¡œ 'all_news' ì‚¬ìš©)
    file_name = f"naver_news_14_{current_time}.csv"
    
    # os.pathë¥¼ ì‚¬ìš©í•˜ì—¬ ê²½ë¡œ ìƒì„±
    output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)))
    file_path = os.path.join(output_dir, file_name)
    
    final_df.to_csv(file_path, index=False, encoding="utf-8-sig")
    print(f"\nğŸ‰ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {file_path}")
    print(f"ì´ {len(final_df)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
else:
    print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# XML í˜•ì‹ìœ¼ë¡œ ìš”ì²­í•˜ë ¤ë©´ ì•„ë˜ URLì„ ì‚¬ìš©
# url = f"https://openapi.naver.com/v1/search/news.xml?query={encoded_query}&display=10&start=1&sort=sim"